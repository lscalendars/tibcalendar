\mychapter{Precision of different computer methods}

In the process of improving precision in calculations, it is very important to understand the limits of the underlying hardware and software, and the cost of going over them.

\mysection{Different computer representations}

\mysubsection{General representation}

Real numbers are represented by floating point numbers, that can be schematized as:
\begin{itemize}
\item a sign, encoded on 1 bit
\item an exponent $e$, encoded in several bits (11 on double precision)
\item a significand $s$, encoded in the rest of the bits (52 on double precision), these bits are called the mantissa
\end{itemize}

In arbitrary precision libraries, operations are made in software and thus doesn't depend too much on hardware representation, at the cost of an important speed loss. The system is the same though, but the size of the exponent and the mantissa is given by the user\footnote{Note that the mainstream GMP/MPFR doesn't allow to set truely arbitrary exponent}.

The representation size of a floating point number vary according to hardware architecture and user input. We will take into consideration here the most common types (which should cover 99.99\% of use cases). We describe here the IEEE-754 standard as implemented in C99, but there are strict equivalents in Fortran.

\mysubsection{Norms and definitions}

This paragraph describes the most simple description of floats, namely the description of the main types as in the IEEE-754 norm. This can be summarized in the following table:

\begin{center}
\vspace{\spacearoundtables}
\begin{tabular}{|l|S|S|S|}
\hline
\multicolumn{1}{|c|}{\textbf{name}} & \multicolumn{1}{c|}{\textbf{total}} & \multicolumn{1}{c|}{\textbf{exponent}} & \multicolumn{1}{c|}{\textbf{mantissa}} \\\hline
Single precision & 32 & 8 & 23 \\\hline
Double precision & 64 & 11 & 52 \\\hline
Quadruple precision & 128 & 15 & 112 \\\hline
\end{tabular}
\vspace{\spacearoundtables}
\end{center}

But as we will see, there are some discrepancies between this and reality, due to several implementation choices that we will detail in the next sections.

\mysubsection{Hardware implementation}

There is currently no widespread hardware that can make operations on floating points with more than 80 bits\footnote{though the z/Architecture of IBM Mainstream Servers can compute 128-bit float operations.}, and this limitation tends to be more and more restrictive. This section will explain the different hardware implementations. It is a very important topic to understand as it is non-obvious and implementation choices might seem very strange.

First we can distinguish between two floating point computing hardwares:
\begin{enumerate}
\item[FPU]\footnote{\emph{Floating Point Unit}, or x87.} This is the historical hardware introduced by Intel, present in all x86 and x86\_64 computers. This hardware has 80-bits registers and thus cannot make more precise operations.
\item[SIMD]\footnote{\emph{Single Instruction, Multiple Data}.} Single operations work with at most 64-bit registers and are thus less precise. SIMD offers thought the possibility to make several operations at the same time, hence the trend to prefer it. SSE, AVX, NEON, etc. are all following this architecture. Most complex operations such as trigonometric ones are not implemented and fall back on \emph{FPU}.
\end{enumerate}

Floating point numbers represented in 80-bit registers follow the \emph{extended precision} of IEEE-754 standards and is composed of a 15-bit exponent and a 64-bits mantissa.

Modern x86 and x86\_64 hardware have both SIMD (SSE or AVX) and FPU.

On decent compilers, it is possible to decide between SIMD, FPU or both\footnote{on \texttt{gcc}, the \texttt{-mfpmath} switch allows this, see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86_002d64-Options.html} and \url{http://gcc.gnu.org/wiki/Math_Optimization_Flags}}. It is thus necessary to study this topic if you want to work on precision. By default, on x86 processors, the FPU is used, while SIMD is the default on x86\_64 processors and ARM processors.

It is important to note that on ARM processors (present on smartphones, tablets, etc.), floating point operations are sometimes done in a VFP, a kind of modified FPU. This VFP has no clear specification, but it seems not to have 80-bit registers, so smartphones and tablets should be considered to have only 64-bit registers. Also, VFP is not mandatory in the ARM architecture, and can be replaced by a software fallback (softvp)\footnote{see \url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0348c/CIHCJIFE.html}} so the target architecture is definitely to be inspected carefully!

Another form of harware implementation was present in PowerPC and SPARC processors, where 128-bit numbers could be encoded in two 64-bit registers with operations combining them. This arithmetic had a precision of about 106-bit.

\mysubsection{Software fallback}

It is possible to make floating point arithmetic with software, at the cost of very significant speed reduction. These implementations are of two types:

\begin{enumerate}
\item[\enumstyle{Compiler}] Sometimes languages specifications (or extensions) cannot be implemented with hardware, and thus compilers provide a software fallback for some types
\item[\enumstyle{Libraries}] Some libraries can do arbitrary precision floating point arithmetic. The most famous ones being GMP\footnote{\url{http://gmplib.org/}} and MPFR\footnote{\url{http://www.mpfr.org/}}.
\end{enumerate}

\mysubsection{C implementation}

This part will describe C99, but anyone using a language for astronomical calculations should get knowledge on this topic for the language he chooses.

The discrepancy between hardware and a naive float approach is naturally also present between the naive approach and the various implementations of compilers. Indeed, compilers tend to stick as close to hardware as possible (which is their role) in order to use as few costy software emulation as possible. 

As we've seen, almost no hardware is capable of more than 80-bit calculations, thus hardware 128-bit calculation is impossible. We can also safely assume that users using the C99 keyword \texttt{long double} don't want software emulation. To solve this dilemma, compilers chose different options, the main things to know being summed up in the next paragraphs.

\mysubsubsection{\texttt{double} on FPU}

\texttt{double} keyword always uses 64-bit representation in memory, but when it comes to representation in (80-bit) FPU registers, two behaviours are possible:
\begin{enumerate}
\item[\enumstyle{80-bit mode}] calculations and intermediate values are made in 80-bit, which improves the precision of the calculations\footnote{but may lead in wrong comparaison results, see \url{http://gcc.gnu.org/wiki/x87note}}
\item[\enumstyle{64-bit mode}] the mantissa of intermediate values is rounded to 53 bits (thus giving double precision precision)\footnote{This is what the \texttt{-mpc64} option of gcc or the \texttt{/fp:precise} option of MSVC do, see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86\_002d64-Options.html} and \url{http://msdn.microsoft.com/en-us/library/e7s85ffb.aspx}.}.
\end{enumerate}

In C99, it is possible to know the intermediate rounding of operations with the C99 macro \texttt{FLT\_EVAL\_METHOD}\footnote{\url{http://pubs.opengroup.org/onlinepubs/009695399/basedefs/math.h.html}}.

\texttt{double} on ARM is 64 bytes and always computed on VFP. It's important to note here that the NEON instructions don't provide double precision operations\footnote{see \url{http://www.arm.com/products/processors/technologies/neon.php}}.

\mysubsubsection{\texttt{long double} on FPU}

\texttt{long double} can have several representations:
\begin{enumerate}
\item[\enumstyle{64-bit}] this is the default on MSVC, \texttt{long double} being a synonym of \texttt{double}\footnote{see \url{http://msdn.microsoft.com/en-us/library/9cx8xs15.aspx}}
\item[\enumstyle{96-bit}] this is the default on gcc. In this case, 16 bits are not used in calculations in 80-bit mode.
\item[\enumstyle{128-bit}] this is the case on gcc with the \texttt{-m128bit-long-double}\footnote{see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86\_002d64-Options.html}}. 48 bits are not used in calculations.
\end{enumerate}

For the sake of completeness, it's important to notice here that gcc allows \texttt{\_\_float80}\footnote{\url{http://gcc.gnu.org/onlinedocs/gcc/Floating-Types.html}} type which is a synonym of \texttt{long double} on x86 and x86\_64 architectures.

\mysubsubsection{\texttt{long double} on SIMD and ARM}

The x86\_64 ABI\footnote{\url{http://www.x86-64.org/documentation/abi.pdf}} states that \texttt{long double} has intermediate values and calculations is in extended precision (80-bit), all operations being performed on the FPU; so no SIMD code will be used with the keyword \texttt{long double}.

On ARM, \texttt{long double} is a synonym of double \footnote{see \url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0067d/BABFCGFC.html}}.

\mysubsubsection{Summing up}

We can thus construct the following table summing up the previous paragraphs, giving some name conventions we'll use later. These names are just convenient and (though largely the same) not identical to IEEE-754 standard. The number of bits cells are in the form $x$/$y$, where $x$ is the number of bits used in memory and $y$ the number of bits used in calculations and intermediate values.

%\begin{table}[h]
%\begin{tabu} to \linewidth{|X[l]|X[c]|X[c]|X[c]|}
%\hline
%\rowfont[c]{\bfseries} Compiler & SIMD & x86 FPU & x86\_64 FPU
%\\\hline
%\textbf{GCC} & 32/32 & 32/80 & 32/80\\\hline
%\textbf{MSVC} & 32/32 & 32/80 & 32/80\\\hline 
%\end{tabu}
%\caption{\texttt{float} representation}
%\end{table}

%\begin{table}[h]
%\begin{tabu} to \linewidth{|X[l]|X[c]|X[c]|X[c]|}
%\hline
%\rowfont[c]{\bfseries} Compiler & SIMD & x86 FPU & x86\_64 FPU
%\\\hline
%\textbf{GCC} & 64/64 & 64/80 & 64/80\\\hline
%\textbf{MSVC} & 64/64 & 64/80 & 64/80\\\hline
%\end{tabu}
%\caption{\texttt{double} representation}
%\end{table}

%\begin{table}[h]
%\begin{tabu} to \linewidth{|X[l]|X[c]|X[c]|X[c]|}
%\hline
%\rowfont[c]{\bfseries} Compiler & SIMD & x86 FPU & x86\_64 FPU
%\\\hline
%\textbf{GCC} & 64/64 & 96/80 & 128/80\\\hline
%\textbf{MSVC} & 64/64 & 64/80 & 64/80\\\hline
%\end{tabu}
%\caption{\texttt{long double} representation}
%\end{table}

\begin{table}[h]
\begin{tabu}{|X[l]|X[c]|X[c]|X[c]|}
\hline
\rowfont[c]{\bfseries} Architecture / Compiler & float & double & long double
\\\hline
\textbf{SSE} & 32/32 ($\times$4) & 64/64 ($\times$2) & 64/64 ($\times$2)\\\hline
\textbf{AVX} & 32/32 ($\times$8) & 64/64 ($\times$4) & 64/64 ($\times$4)\\\hline
\textbf{NEON} & 32/32 ($\times$2) & error & error\\\hline
\textbf{ARM VFP} & 32/32 & 64/64 & 64/64\\\hline
\textbf{x86 FPU (gcc)} & 32/80 & 64/80 & 96/80\\\hline
\textbf{x86\_64 FPU (gcc)} & 32/80 & 64/80 & 128/80\\\hline
\textbf{x86 or x86\_64 FPU (msvc)} & 64/80 & 64/80 & 64/80\\\hline
\end{tabu}
\caption{C floating point types representation on different architectures}
\end{table}

%\vspace{\spacearoundtables}

%\begin{tabu} to \linewidth{|X[2]|X[c]|X[4]|}
%\hline
%\rowfont[c]{\bfseries} Name & Nb. of bits & Name in C
%\\\hline
%Simple precision & 32/32 & \texttt{float} in SIMD\\\hline
%Mixed simple precision & 32/80 & \texttt{float} in FPU\\\hline
%Double precision & 64/64 & \texttt{double}, \texttt{long double} on ARM and modern instruction sets (SSE, AVX). If on FPU, must be set in 64-bit mode.\\\hline
%Extended double precision & 80/80 & not available as such (only internal registers) \\\hline
%Mixed double precision & 64/80 & \texttt{double} in FPU, also \texttt{long double} on MSVC \\\hline
%Mixed triple precision & 96/80 & \texttt{long double} on x86 FPU\\\hline
%Quadruple precision & 128/128 & \texttt{\_\_float128} of \texttt{gcc}, software emulation except on a few hardwares\\\hline
%Mixed quadruple precision & 128/80 & \texttt{long double} on x86\_64 FPU\\\hline
%\end{tabu}

%\vspace{\spacearoundtables}

%This table is another view of the same data:

%\vspace{\spacearoundtables}

%\begin{tabu} to \linewidth{|X|X[c]|X[c]|X[c]|X[c]|}
%\hline
%\rowfont[c]{\bfseries} C keyword & FPU (64-bit) & FPU (80-bit) & SIMC & MSVC
%\\\hline
%\texttt{float} & 32/64 & 32/80 & 32/32 & no difference\\\hline
%\texttt{double} & 64/64 & 64/80 & 64/64 & no difference \\\hline
%\texttt{long double} & 96/80 or 128/80 & 96/80 or 128/80 & idem \texttt{double} on ARM, mapped to FPU with others & idem \texttt{double} \\\hline
%\texttt{\_\_float128} & 128/128 & 128/128 or 128/128 & error \\\hline
%\end{tabu}

%\vspace{\spacearoundtables}

\mysection{Common errors due to floating point representations}

This section is an overview of the most common errors due to floating point arithmetic, and of their solution.

\mysubsection{Introduction}

One of the problems of floating point arithmetic is that global formulas are almost inexistant and error for each floating point number manipulation should be calculated by hand, depending on the variable maxima and minima, the chosen float representation, etc.

This section will thus describe only general errors and things to know about floating point manipulation. A good introduction to this topic is \cite{Goldberg}, and \cite{Higham} a more recent and complete book. This section describes the general principles described in these.

\mysubsubsection{Notation}

We will use here analytical notation of floating points numbers we can find very commonly. We will represent a floating point number as being in the form $$d.dd...dd\times\beta^e$$where $d.dd...dd$ is the significand and has p digits, $\beta$ is the base (assumed to be even) and $e$ the exponent.

To make the link with computer representations, we would have:
\begin{itemize}
\item $\beta=2$
\item $p$ equal to the number of bits in the mantissa
\item $d.dd...dd$ the mantissa, with d in base 2, for example 1.100110011001100
\item $e$ the exponent
\end{itemize}

\mysubsubsection{Non-} % communativit√©

\mysubsection{Polynomial calculations}

When evaluating polynomial expressions, which are numerous in astronomical calculations, the standard method is to use Horner's method. A very simple example will show why:

If you evaluate $$3x^3 + 2x^2 + 4x + 5$$ in this very form, 10 operations will be performed (7 multiplications and 3 sums). But if you evaluate it as $$((3x + 2)x + 4)x + 5$$, only 6 operations (3 multiplications and 3 sums) are needed.

This operation reducing implies a gain in performance and also naturally in precision: the less operations, the less errors!

For further reading and error bouding, see setion 5.1 of \cite{Higham}.

\mysubsection{Fused Multiply-Add}

\mysubsubsection{Principles}

Fused Multiply-Add (FMA) is a hardware instruction that computes a multiplication and an addition. The interesting thing is that IEEE 754-2008 standard\footnote{see section 5.4.1 of \cite{IEEE754}} specifies that the whole operation should be performed as if no rounding occured, rounding only once at the end. This means that the whole operation is precise to $\frac{1}{2}\,ula$, where a multiplication then a sum would cumulate roundings and thus be precise only to $1\,ula$. There is thus, like with Horner's method, a gain in both performance and precision.

\mysubsubsection{Real-life use}

FMA are sadly not really widespread. Here is what seems to be the current situation:
\begin{itemize}
\item x87 doesn't handle it\footnote{except for a patented extension (patent US7499962, see \href{http://www.google.com/patents/US7499962}), which no common hardware seems to implement.}
\item AVX have FPA as an extension, thus only recent Intel hardware (as of end of 2013) will be able to use it\footnote{see \url{http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html}\TODO}
\item ARM's VFP and NEON seem to handle FPA\footnote{see\url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0056d/Bcfhgcgd.html}}
\end{itemize}

The FMA had two mutually incompatible implementations, one on three operands (FMA3) and one on four operands (FMA4), each having made its way in AMD and Intel processors at different times\footnote{\url{The only source I found for this is \url{http://en.wikipedia.org/wiki/FMA_instruction_set#History}}. Thus FMA instructions will be computed by gcc only if the targer architecture is specified (or the specific \texttt{-mfma} and \texttt{-mfma4} switches). It is thus hard to make it generic\footnote{Though the very exciting gcc feature of Function Multiversionning might help a lot, see \url{http://gcc.gnu.org/onlinedocs/gcc/Function-Multiversioning.html}.}.

% http://shemesh.larc.nasa.gov/NFM2010/papers/nfm2010_14_23.pdf
% http://hal.inria.fr/docs/00/77/25/08/PDF/main.pdf
% http://frama-c.com/

Except on very specific hardwares, it is thus sadly impossible to get FPA on extended double precision. If you manage to use it, it's important to note that using FPA requires extra care about operations order and compiler optimization (see 2.6 in \cite{Higham}).

\mysubsection{Summations}

Several optimizations for sums are possible, we'll describe the most simples. An excellent reading on the topic is chapter 4 of \cite{Higham}.

\mysubsubsection{Recursive summation}

Before seing how to optimize precision, let's present the precision of what is often called \emph{recursive summation}

\mysubsubsection{Ascending order}

It is a good practice, for sums of more than two floating point numbers, to sum in ascending orders, treating small numbers before big ones. The reason for this is fairly simple and explained in 

\mysubsubsection{Compensated Summation}

\mysubsubsection{Other efficient exact sums}

\cite{Langlois} % Langlois http://hal.archives-ouvertes.fr/docs/00/73/76/17/PDF/hal-scico12.pdf



\mysubsection{Errors in transcendental functions}

Transcendental functions (like sin and cos) are not mandatorily exactly rounded.

% Section 8.3.8 of  http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html
% says 1ulp precision when rounding to nearest even

\mysubsection{Errors induced by optimizers}

Reordering operations

algebrical simplifications

\mysection{Errors induced by floating point in common astronomical calculations}

\mysubsection{Converting floating point error in angle precision error}

\mysubsection{Errors in common astronomical calculations}

\mysubsubsection{Theorical bounds}

\mysubsubsection{Some measures}

% TODO: vector code less precise? http://software.intel.com/sites/default/files/article/326703/floating-point-differences-sept11_0.pdf p.2
% TODO: transcendental functions: 4ulp??? http://software.intel.com/sites/default/files/article/326703/floating-point-differences-sept11_0.pdf
% http://lipforge.ens-lyon.fr/www/crlibm/

% http://gcc.gnu.org/gcc-4.3/changes.html#mpfropts << what is this? OK: for constants, evaluated at compile-time << constant folding
