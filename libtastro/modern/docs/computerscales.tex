\mychapter{Precision of different computer methods}

In the process of improving precision in calculations, it is very important to understand the limits of the underlying hardware and software, and the cost of going over them.

\mysection{Different computer representations}

\mysubsection{General representation}

Real numbers are represented by floating point numbers, that can be schematized as:
\begin{itemize}
\item a sign, encoded on 1 bit
\item an exponent $e$, encoded in several bits (11 on double precision)
\item a significand $s$, encoded in the rest of the bits (52 on double precision), these bits are called the mantissa
\end{itemize}

In arbitrary precision libraries, operations are made in software and thus doesn't depend too much on hardware representation, at the cost of an important speed loss. The system is the same though, but the size of the exponent and the mantissa is given by the user\footnote{Note that the mainstream GMP/MPFR doesn't allow to set truely arbitrary exponent}.

The representation size of a floating point number vary according to hardware architecture and user input. We will take into consideration here the most common types (which should cover 99.99\% of use cases). We describe here the IEEE-754 standard as implemented in C99, but there are strict equivalents in Fortran.

\mysubsection{Norms and definitions}

This paragraph describes the most simple description of floats, namely the description of the main types as in the IEEE-754 norm. This can be summarized in the following table:

\begin{center}
\vspace{\spacearoundtables}
\begin{tabular}{|l|S|S|S|}
\hline
\multicolumn{1}{|c|}{\textbf{name}} & \multicolumn{1}{c|}{\textbf{total}} & \multicolumn{1}{c|}{\textbf{exponent}} & \multicolumn{1}{c|}{\textbf{mantissa}} \\\hline
Single precision & 32 & 8 & 23 \\\hline
Double precision & 64 & 11 & 52 \\\hline
Quadruple precision & 128 & 15 & 112 \\\hline
\end{tabular}
\vspace{\spacearoundtables}
\end{center}

But as we will see, there are some discrepancies between this and reality, due to several implementation choices that we will detail in the next sections.

\mysubsection{Hardware implementation}

There is currently no widespread hardware that can make operations on floating points with more than 80 bits\footnote{though the z/Architecture of IBM Mainstream Servers can compute 128-bit float operations.}, and this limitation tends to be more and more restrictive. This section will explain the different hardware implementations. It is a very important topic to understand as it is non-obvious and implementation choices might seem very strange.

First we can distinguish between two floating point computing hardwares:
\begin{enumerate}
\item[FPU]\footnote{\emph{Floating Point Unit}, or x87.} This is the historical hardware introduced by Intel, present in all x86 and x86\_64 computers. This hardware has 80-bits registers and thus cannot make more precise operations.
\item[SIMD]\footnote{\emph{Single Instruction, Multiple Data}.} Single operations work with at most 64-bit registers and are thus less precise. SIMD offers thought the possibility to make several operations at the same time, hence the trend to prefer it. SSE, AVX, NEON, etc. are all following this architecture. Most complex operations such as trigonometric ones are not implemented and fall back on \emph{FPU}.
\end{enumerate}

Floating point numbers represented in 80-bit registers follow the \emph{extended precision} of IEEE-754 standards and is composed of a 15-bit exponent and a 64-bits mantissa.

Modern x86 and x86\_64 hardware have both SIMD (SSE or AVX) and FPU.

On decent compilers, it is possible to decide between SIMD, FPU or both\footnote{on \texttt{gcc}, the \texttt{-mfpmath} switch allows this, see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86_002d64-Options.html} and \url{http://gcc.gnu.org/wiki/Math_Optimization_Flags}}. It is thus necessary to study this topic if you want to work on precision. By default, on x86 processors, the FPU is used, while SIMD is the default on x86\_64 processors and ARM processors.

It is important to note that on ARM processors (present on smartphones, tablets, etc.), floating point operations are sometimes done in a VFP, a kind of modified FPU. This VFP has no clear specification, but it seems not to have 80-bit registers, so smartphones and tablets should be considered to have only 64-bit registers. Also, VFP is not mandatory in the ARM architecture, and can be replaced by a software fallback (softvp)\footnote{see \url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0348c/CIHCJIFE.html}} so the target architecture is definitely to be inspected carefully!

Another form of harware implementation was present in PowerPC and SPARC processors, where 128-bit numbers could be encoded in two 64-bit registers with operations combining them. This arithmetic had a precision of about 106-bit.

\mysubsection{Software fallback}

It is possible to make floating point arithmetic with software, at the cost of very significant speed reduction. These implementations are of two types:

\begin{enumerate}
\item[\enumstyle{Compiler}] Sometimes languages specifications (or extensions) cannot be implemented with hardware, and thus compilers provide a software fallback for some types
\item[\enumstyle{Libraries}] Some libraries can do arbitrary precision floating point arithmetic. The most famous ones being GMP\footnote{\url{http://gmplib.org/}} and MPFR\footnote{\url{http://www.mpfr.org/}}.
\end{enumerate}

\mysubsection{C implementation}

This part will describe C99, but anyone using a language for astronomical calculations should get knowledge on this topic for the language he chooses.

The discrepancy between hardware and a naive float approach is naturally also present between the naive approach and the various implementations of compilers. Indeed, compilers tend to stick as close to hardware as possible (which is their role) in order to use as few costy software emulation as possible. 

As we've seen, almost no hardware is capable of more than 80-bit calculations, thus hardware 128-bit calculation is impossible. We can also safely assume that users using the C99 keyword \texttt{long double} don't want software emulation. To solve this dilemma, compilers chose different options, the main things to know being summed up in the next paragraphs.

\mysubsubsection{\texttt{double} on FPU}

\texttt{double} keyword always uses 64-bit representation in memory, but when it comes to representation in (80-bit) FPU registers, two behaviours are possible:
\begin{enumerate}
\item[\enumstyle{80-bit mode}] calculations and intermediate values are made in 80-bit, which improves the precision of the calculations\footnote{but may lead in wrong comparaison results, see \url{http://gcc.gnu.org/wiki/x87note}}
\item[\enumstyle{64-bit mode}] the mantissa of intermediate values is rounded to 53 bits (thus giving double precision precision)\footnote{This is what the \texttt{-mpc64} option of gcc or the \texttt{/fp:precise} option of MSVC do, see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86\_002d64-Options.html} and \url{http://msdn.microsoft.com/en-us/library/e7s85ffb.aspx}.}.
\end{enumerate}

The FPU can also have a 32-bit mode based on the same principles as the 64-bit mode. Compilers Macros allow to change FPU mode during runtime. This is the case of \texttt{\_controlfp} of MSVC\footnote{\url{http://msdn.microsoft.com/en-us/library/e9b52ceh\%28VS.71\%29.aspx}} and \texttt{\_FPU\_SETCW} of gcc{see \texttt{man 3 \_\_setupcw}}.

\texttt{double} on ARM has a 64 bit representation and is always computed on VFP. It's important to note here that the NEON instructions don't provide double precision operations\footnote{see \url{http://www.arm.com/products/processors/technologies/neon.php}}.

\mysubsubsection{\texttt{long double} on FPU}

\texttt{long double} can have several representations:
\begin{enumerate}
\item[\enumstyle{64-bit}] this is the default on MSVC, \texttt{long double} being a synonym of \texttt{double}\footnote{see \url{http://msdn.microsoft.com/en-us/library/9cx8xs15.aspx}}
\item[\enumstyle{96-bit}] this is the default on gcc. In this case, 16 bits are not used in calculations in 80-bit mode.
\item[\enumstyle{128-bit}] this is the case on gcc with the \texttt{-m128bit-long-double}\footnote{see \url{http://gcc.gnu.org/onlinedocs/gcc/i386-and-x86\_002d64-Options.html}}. 48 bits are not used in calculations.
\end{enumerate}

For the sake of completeness, it's important to notice here that gcc allows \texttt{\_\_float80}\footnote{\url{http://gcc.gnu.org/onlinedocs/gcc/Floating-Types.html}} type which is a synonym of \texttt{long double} on x86 and x86\_64 architectures.

\mysubsubsection{\texttt{long double} on SIMD and ARM}

The x86\_64 ABI\footnote{\url{http://www.x86-64.org/documentation/abi.pdf}} states that \texttt{long double} has intermediate values and calculations is in extended precision (80-bit), all operations being performed on the FPU; so no SIMD code will be used with the keyword \texttt{long double}.

On ARM, \texttt{long double} is a synonym of double\footnote{see \url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0067d/BABFCGFC.html}}.

\mysubsubsection{Summing up}

We can thus construct the following table summing up the previous paragraphs, giving some name conventions we'll use later. These names are just convenient and (though largely the same) not identical to IEEE-754 standard. The number of bits cells are in the form $x$/$y$, where $x$ is the number of bits used in memory and $y$ the number of bits used in calculations and intermediate values. When a "($\times x$)" is present, $x$ indicates the number of simultaneous operations on the type the architecture is capable of.

\begin{table}[h]
\begin{tabu}{|X[l]|X[c]|X[c]|X[c]|}
\hline
\rowfont[c]{\bfseries} Architecture / Compiler & float & double & long double
\\\hline
\textbf{SSE} & 32/32 ($\times$4) & 64/64 ($\times$2) & 64/64 ($\times$2)\\\hline
\textbf{AVX} & 32/32 ($\times$8) & 64/64 ($\times$4) & 64/64 ($\times$4)\\\hline
\textbf{NEON} & 32/32 ($\times$2) & unavailable & unavailable\\\hline
\textbf{ARM VFP} & 32/32 & 64/64 & 64/64\\\hline
\textbf{x86 FPU (gcc)} & 32/80 & 64/80 & 96/80\\\hline
\textbf{x86\_64 FPU (gcc)} & 32/80 & 64/80 & 128/80\\\hline
\textbf{x86 or x86\_64 FPU (msvc)} & 32/80 & 64/80 & 64/80\\\hline
\end{tabu}
\caption{C floating point types representation on different architectures}
\end{table}

\mysection{Common errors due to floating point representations}

This section is an overview of the most common errors due to floating point arithmetic, and of their solution.

\mysubsection{Introduction}

One of the problems of floating point arithmetic is that global formulas are almost inexistant and error for each floating point number manipulation should be calculated by hand, depending on the variable maxima and minima, the chosen float representation, etc.

This section will thus describe only general errors and things to know about floating point manipulation. A good introduction to this topic is \cite{Goldberg}, and \cite{Higham} a more recent and complete book. This section describes the general principles described in these.

\mysubsubsection{Notation}

We will use here analytical notation of floating points numbers we can find very commonly. We will represent a floating point number as being in the form $$d.dd...dd\times\beta^e$$where $d.dd...dd$ is the significand and has p digits, $\beta$ is the base (assumed to be even) and $e$ the exponent.

To make the link with computer representations, we would have:
\begin{itemize}
\item $\beta=2$
\item $p$ equal to the number of bits in the mantissa
\item $d.dd...dd$ the mantissa, with d in base 2, for example 1.100110011001100
\item $e$ the exponent
\end{itemize}

\mysubsubsection{Errors of floating points}\label{ulp}

The errors introduced by floating point are usually given in the \emph{unit in the last place} (ulp) unit. To understand it, let's take an example with $\beta=10, p=3, e_{max}=4$. For the number $1.04\times 10^{-2}$, the ulp is $0.01\times 10^{-2} = 1.10^{-4}$, meaning the smallest number representable with the same $\beta$, $p$ and $e$. This unit is used in measures of precision of operations. For example, IEEE-754 makes it mandatory for the sum operations to be accurate at $\frac{1}{2}ulp$, which means that when computing $z=x+y$, if $z_r$ is the real result and $z_f$ is the floating point number returned by the function, we have $z_r\in[z_f-\frac{1}{2}ulp(z), z_f-\frac{1}{2}ulp(z)]$.

In general, the ulp can be expressed as $\beta^{-(p-1)+e}$, so for floating point on computers, we have $1\,ulp=2^{e-p+1}$ and $\frac{1}{2}ulp=2^{e-p}$.

% MULLER: http://ljk.imag.fr/membres/Carine.Lucas/TPScilab/JMMuller/ulp-toms.pdf

It is possible in C99 to get an interesting quantity corresponding most of the time to $ulp(x)$ with the code \texttt{nextafter(x, DBL\_MAX);}\footnote{see \texttt{man 3 nextafter}}.

\mysubsubsection{Algebric properties} % communativit√©

A difficulty we'll discuss further later (\ref{arithmeticreduction} and \ref{summations}) is the lack of good arithmetic properties of floating point operations. They of course have almost the same properties as their corresponding real number operations, but when precision counts these properties don't apply anymore.

Maybe the most unnatural property is the fact that \emph{floating point operations are not associative}. For instance $$(0.1 + 0.2) + 0.3 \neq 0.1 + (0.2 + 0.3)$$\footnote{Section \ref{ascendingorder} explains why in details}. So operation order shall be carefully chosen.

Also, \emph{floating point operations are not distributive}: $$0.1 \times (0.3 - 0.4) \neq (0.1 \times 0.3) - (0.1 \times 0.4)$$.

For all these reasons, one must really be careful with comparisons, as rounding errors will make them unrelevant.

This leads in difficulties in operations ordering, and this is mostly what the rest of the section is about.

\mysubsection{Polynomial calculations}\label{polynomials}

When evaluating polynomial expressions, which are numerous in astronomical calculations, the standard method is to use Horner's method. A very simple example will show why:

If you evaluate $$3x^3 + 2x^2 + 4x + 5$$ in this very form, 10 operations will be performed (7 multiplications and 3 sums). But if you evaluate it as $$((3x + 2)x + 4)x + 5$$, only 6 operations (3 multiplications and 3 sums) are needed.

This operation reducing implies a gain in performance and also naturally in precision: the less operations, the less errors!

For further reading and error bouding, see setion 5.1 of \cite{Higham}.

\mysubsection{Fused Multiply-Add}\label{fma}

\mysubsubsection{Principles}

Fused Multiply-Add (FMA) is a hardware instruction that computes a multiplication and an addition. The interesting thing is that IEEE 754-2008 standard\footnote{see section 5.4.1 of \cite{IEEE754}} specifies that the whole operation should be performed as if no rounding occured, rounding only once at the end. This means that the whole operation is precise to $\frac{1}{2}\,ula$, where a multiplication then a sum would cumulate roundings and thus be precise only to $1\,ula$. There is thus, like with Horner's method, a gain in both performance and precision.

\mysubsubsection{Real-life use}

FMA are sadly not really widespread. Here is what seems to be the current situation:
\begin{itemize}
\item x87 doesn't handle it\footnote{except for a patented extension (patent US7499962, see \href{http://www.google.com/patents/US7499962}), which no common hardware seems to implement.}
\item AVX have FPA as an extension, thus only recent Intel hardware (as of end of 2013) will be able to use it\footnote{see \url{http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html}\TODO}
\item ARM's VFP and NEON seem to handle FPA\footnote{see\url{http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0056d/Bcfhgcgd.html}}
\end{itemize}

The FMA had two mutually incompatible implementations, one on three operands (FMA3) and one on four operands (FMA4), each having made its way in AMD and Intel processors at different times\footnote{The only source I found for this is \url{http://en.wikipedia.org/wiki/FMA\_instruction\_set\#History}}. Thus FMA instructions will be computed by gcc only if the targer architecture is specified (or the specific \texttt{-mfma} and \texttt{-mfma4} switches). It is thus hard to make it generic\footnote{Though the very exciting gcc feature of Function Multiversionning might help a lot, see \url{http://gcc.gnu.org/onlinedocs/gcc/Function-Multiversioning.html}.}.

Except on very specific hardwares, it is thus sadly impossible to get FPA on extended double precision. If you manage to use it, it's important to note that using FPA requires extra care about operations order and compiler optimization (see 2.6 in \cite{Higham}).

\mysubsection{Summations}\label{summations}

Several optimizations for sums are possible, we'll describe the most simples. An excellent reading on the topic is chapter 4 of \cite{Higham}. This topic is quite important, as astronomical calculations often sum large amounts of numbers (more than a thousand).

\mysubsubsection{Recursive summation}

Before seing how to optimize precision, let's present the precision of what is often called \emph{recursive summation}, meaning the naive approach of summing each term consecutively.

If we call $n$ the number of terms in the summation, the bound of error of recursive summation is $n\times\epsilon$ where $\epsilon$ is the error bound of one summation, $0.5\,ulp$ with IEEE-754 compliant systems. This means $n\times0.5\times2^{e-1023} = n\times2^{e-1022}$.

To take a realistic example in astronomical calculations, let's take $n=1024=2^{10}$ and let's say we're working on 64bit registers, with values ranging around $0.5$, meaning $e\approx1022$, we have thus an error bounded by $1$, meaning totally unreliable results!

Of course this is a bound which will most certainly never be reached, but it's definitely something to pay attention to.

A refinement of this value is given in Table 4.1 of  \cite{Higham}.

\mysubsubsection{Ascending order}\ref{ascendingorder}

It is a good practice, for sums of more than two floating point numbers, to sum in ascending order of exponent (if we can know or guess it), treating small numbers before big ones. The reason for this is fairly simple and can be explained by the following example.

Suppose $\beta=10$, $p=3$, $e=4$ (which means we manipule decimal numbers with three digits). Let's add $0.4$, $0.4$ and $100$. Without ascending order, we would have:

$$1.00\times 10^2 + 4.00\times 10^{-1} = 1.00\times 10^2, 1.00\times 10^2 + 4.00\times 10^{-1} = 1.00\times 10^2$$

And thus a result of 100, due to the fact that 1.004 would require a minimum value of $4$ for $p$. Now let's do it in the ascending order:

$$4.00\times 10^{-1} + 4.00\times 10^{-1} = 8.00\times 10^{-1}, 1.00\times 10^2 + 8.00 \times 10^{-1} = 1.01\times 10^2$$

which is much closer. A counter-example is if, in the same system, if you add $100$, $-100$ and $1$ in ascending order, you have:

$$1.00\times 10^{-1} + 1.00\times 10^{2} = 1.00\times 10^{2} - 1.00\times 10^{2} = 0$$

whereas in the descending order:

$$1.00\times 10^{2} - 1.00\times 10^{2} = 0.00\times 10^{0} + 1.00\times 10^{-1} = 1.00\times 10^{-1}$$

which is the true answer.

Descending order is more often more accurate though, and it seems reasonable to use it, especially since it's free if you can guess the exponent order (which is the case in most astronomical calculations).

The gain in accuracy is not very good and not easily predictable though, so this method is rarely documented as a worthy improvement. The mean error table of \cite{Higham} gives an improvement by 3 compared to standard recursive summation.

\mysubsubsection{Paired Summation}

A simple method to really improve the accuracy of calculations is to compute the sum by pairs. For example, the paired computation of $$y=x_1+x_2+x_3+x_4+x_5+x_6$$ is $$y=(x_1+x_2)+(x_3+x_4)+(x_5+x_6)$$

This very simple and straightforward improvement that has an error of $log_2(n)\times\epsilon$ instead of $n\times\epsilon$.

\mysubsubsection{Compensated Summation}\label{compensated}

The method described here was invented in the 60s, and is often known as the Kahan method, from the name of its author, William Kahan. This method gives a value of the summation which is exact by $1\,ulp$, by compensating the error each time. This is a very large improvement and allows the code to be proven.

The method used won't be detailed here as it would take too much space, but it's well documented on the Web\footnote{See for instance \url{http://en.wikipedia.org/wiki/Kahan_summation_algorithm} and \url{http://www.astro.umd.edu/~dcr/Courses/ASTR615/ps2sol/node1.html}}.

The cost is quite big though, the number of operations being multiplied by 4. Results with this method are really noticeable.

For virtually infinite precision accuracy, a "double compensated summation" exists (see Algorithm 4.3 in \cite{Higham}) taking 10 operations per additions, and needing numbers to be sorted before. This method is thus extremely slow.

\mysubsubsection{Other efficient exact sums}

New algorithms have appeared recently to compute long summations exactly, with an efficiency improved a lot. Documentations and implementations of these algorithms are not avaible freely, but an overview can be found in \cite{Langlois}\TODO . % TODO: Langlois http://hal.archives-ouvertes.fr/docs/00/73/76/17/PDF/hal-scico12.pdf

\mysubsubsection{Conclusion}

It's important to define the balance between accuracy and speed, as the range is large between very slow and very accurate functions and fast and inaccurate ones.

Something very important and documented in all books on the topic is to take into account the possibility of increasing precision of the floating point representation. Indeed, compared to single precision, a recursive summation in double precision has only small speed decrease (around $1.5times$) and better accuracy than compensated summation.

\mysubsection{Errors in trigonometrical functions}

Transcendental functions (like sin and cos) guaranteed accuracy cannot be as good as other functions due to the tablemaker's dillema. Section 8.3.10 of \cite{Intel}\TODO specifies that the error cannot exceed 1\,ulp if rounding mode is \emph{nearest even}.

% TODO: Intel:  http://www.intel.com/content/www/us/en/processors/architectures-software-developer-manuals.html

It's important to also notice that, on FPU, the Internal $\pi$ representation used in trigonometric functions has a higher precision than any possible floating point hardware representation (it has a 66bit mantissa\footnote{See \cite[Intel], 8.3.8.}). Thus moduling an angle by $2\pi$ to put it in the range will be less precise than letting the FPU do by itself.

\mysubsection{Errors induced by compilers}

Something to be very cautious about is compiler optimizations. While these are necessary to improve speed, they can sometimes break precision.

\mysubsubsection{Constants computation}

On some compilers, litteral constant evaluation (for example \texttt{cos(1.65)}) was made with the precision of the constant, and thus could hold errors. This was fixed in gcc in 4.3 version which now uses arbitrary precision\footnote{see \url{http://gcc.gnu.org/gcc-4.3/changes.html\#mpfropts}}. It is thus important to check your compiler before doing any such thing; and the best way to get portable code is to write already evaluated.

\mysubsubsection{Fast maths}

Most compilers can compile code that will run faster at the cost of precision loss. These optimizations make computations not follow the IEEE-754 standard strictly and thus give unpredictable precision. This is the case of the \texttt{-Ofast}, \texttt{-ffast-math} and \texttt{-funsafe-math-optimizations} options of gcc. These would be useful in case of speed optimization.

\mysubsubsection{Arithmetic reduction}\label{arithmeticreduction}

The most dangerous compiler optimization is arithmetic reduction, as it might break precision optimizations. The principle is that compilers will try to treat operations as operations on real numbers, and thus consider them distributive and associative. At a certain optimization level, it will even totally kill compensated summation (see \ref{compensated})\footnote{by a process described in \url{http://en.wikipedia.org/wiki/Kahan_summation_algorithm\#Computer_languages} and easily noticeable on a test.}.

Decent compilers allow to to disable optimization for one function though\footnote{for gcc, see \texttt{optimize} function attribute and \texttt{pragma GCC optimize}, \url{http://gcc.gnu.org/onlinedocs/gcc/Function-Attributes.html} and \url{http://gcc.gnu.org/onlinedocs/gcc/Function-Specific-Option-Pragmas.html\#Function-Specific-Option-Pragmas}}.

\mysection{Errors induced by floating point in common astronomical calculations}

For this part, we'll consider the following most common floating point types:

\begin{itemize}
\item \emph{Single precision} 32-bit representation, 32-bit operations
\item \emph{Mixed single precision} 32-bit representation, 80-bit operations
\item \emph{Double precision} 64-bit representation, 64-bit operations
\item \emph{Mixed double precision} 64-bit representation, 80-bit operations
\item \emph{Extended precision} at least 80-bit representation, 80-bit operations (case of long double on x87 with gcc)
\item \emph{Quadruple precision} 128-bit representation, 128-bit operations
\end{itemize}

For extended precision, we will consider the values to be 80-bits, as these are all other additional bits are garbage.

We'll consider also that computed values will be in radiants.

For final result values, we'll consider them to be and have have an absolute value greater than 0.5 and less than $\pi$, this allows to say that the exponent will be between -1 and 2. The maximum exponent ($e$) and the mantissa size ($p$) for the different representations in memory is:

\begin{table}[h]
\centering
\begin{tabular}{|c|S[table-format=5.0]|S[table-format=3.0]|}
\hline
\textbf{Nb. of bits} & \multicolumn{1}{c|}{\textbf{$e_{max}$}} & \multicolumn{1}{c|}{\textbf{$p$}} \\\hline
\textbf{32} & 127 & 23\\\hline
\textbf{64} & 1023 & 52\\\hline
\textbf{80} & 16383 & 64\\\hline
\textbf{128} & 16383 & 112\\\hline
\end{tabular}
\caption{Caracteristics of the different floating point formats}
\end{table}

\mysubsection{Converting floating point error in angle precision error}

\mysubsubsection{Maximum accuracy allowed by floating point types}

The very optimistic bound for accuracy of result value is the ulp of the floating point representation. Taking 2 as the exponent, we get an maximal error in radians of $2^{-p+2-1} = 2^{-(p-1)}$. The following table gives this maximal accuracy in seconds:

\begin{table}[h]
\centering
\sisetup{table-format=1.1, table-figures-exponent = 2}
\begin{tabular}{|c|S|}
\hline
\textbf{Nb. of bits} & \multicolumn{1}{c|}{\textbf{Max. accuracy (in s)}}\\\hline
\textbf{32} & 5e-2\\\hline
\textbf{64} & 4.6e-11\\\hline
\textbf{80} & 1e-14\\\hline
\textbf{128} & 4e-29\\\hline
\end{tabular}
\caption{Maximum accuracy of final results}
\end{table}

% rules p. 47 of http://www.cs.berkeley.edu/~wkahan/MktgMath.pdf ?

\mysubsection{Error bounds in common astronomical calculations}

\mysubsubsection{vsop87}

Computing a vsop87 coordinate for one planet at time $t$ involves three operations:

\begin{itemize}
\item \emph{base operation} with the form $a_i\times cos(b_i + c_i \times t)$, where $a_i$, $b_i$ and $c_i$ are given litterals
\item \emph{summations} consisting of big sums of terms computed with \emph{base operations}
\item \emph{polynomials} consisting in evaluating a short ($degree < 6$) polynomial with coefficients being results of previous sums, and variable $t$
\end{itemize}

Let's note $n_1$, $n_2$, $n_3$, $n_4$, $n_5$ the number of terms in the different summations. The number of basic operations is thus $N=n_1+n_2+n_3+n_4+n_5$. 

For calendrical calculations, we consider dates between years -2000 and 4000, $t$ being here in julian millenia (\num{365250} julian days), we can consider: $$2.71 < t < 8.72$$ We also consider that the ask t contains no conversion error.

The number of operations in the polynomial evaluation, if taken with Horner's formula, is 10 (5 additions, 5 multiplications)\footnote{see \ref{polynomial}}. 

The error induced in the summations is bound by $N/2\,ulp$ if made in recusive summation, and around $\frac{1}{2}\,ulp$ if made with a precise summation algorithm (like Kahan)\footnote{see \ref{summations}}.

Now let's compute the error of the base operation. A first important remark: we always have $0<b_i<2\pi$\footnote{because $cos(2\pi+x)=cos(x)$, this bound is always respected in the original files.}. Also, $0<a_i<0.1$ seems always respected. The biggest problem is $c_i$, as this one has very large bounds, approximately $0.01<c_i<5\times10^5$ (and thus its exponent $-4<e_c<13$).

We can take a few steps in evaluating the error bound of one base operation $a_i\times cos(b_i + c_i \times t)$:
\begin{itemize}
\item the bound of the error introduced in the conversion in binary of the decimal constants $a_i$, $b_i$ and $c_i$ are respectively $\frac{1}{2}\,ulp(a_i)$, $\frac{1}{2}\,ulp(b_i)$, $\frac{1}{2}\,ulp(c_i)$
\item the error introduced by the operations of $b_i + c_i \times t$ is bound by $1\,ulp(b_i+c_i\times t)$ or by half of it if computed with an FMA (see \ref{fma})
\item without counting the error on multiplication, the error introduced by $c_i \times t$ is bound by $\frac{t_{max}}{2}=5\,ulp(c_i)$
\item the global error bound of $b_i + c_i \times t$ is thus $ulp(b_i) + 5\,ulp(c_i) + (\frac{1}{2} or 1)\,ulp(b_i+c_i\times t)$ according to the fact that it's made with a FMA or not
\item in the previous error, we can make a few simplifications: $ulp(b_i) \ll ulp(c_i)$, and $ulp(b_i+c_i\times t)\approx ulp(c_i)$. We have thus an error of about (5.5 or 6)\,ulp($c_i$)
\item the error introduced by $cos$ is bound by $1\,ulp$ on most hardwares (we'll consider it's the case).
\item without taking the previous error into consideration, the bound of the error introduced by $cos(\alpha)$ when $\alpha$ has an error of $\epsilon$ (this is called forward error\footnote{see section 1.5 of \cite{Higham}}) is the maximum distance between $cos(\alpha)$ and $cos(\alpha + \epsilon)$. This maximum is attained when the derivate ($sin(\alpha)$) has its maximum, when $\alpha = \frac{\pi}{2}$. The bound is thus $cos(\frac{\pi}{2}+\epsilon)-cos(\frac{\pi}{2}) = -sin(\epsilon) \approx \epsilon$ for small $\epsilon$
\end{itemize}

Now, is 6\,ulp($c_i$) small? We have $ulp(c_i)=2^{e_c-p+1}$\footnote{see \ref{ulp}}, with $-4<e_c<13$. We have thus the following table:

\begin{table}[h]
\centering
\sisetup{table-format=1.1, table-figures-exponent = 2}
\begin{tabular}{|c|S|S|}
\hline
\textbf{Nb. of bits} & \multicolumn{1}{c|}{\textbf{$ulp(c_i)_{min}$}} & \multicolumn{1}{c|}{\textbf{$ulp(c_i)_{max}$}}\\\hline
\textbf{32} & 1.5e-8 & 2e-3 \\\hline % p=23
\textbf{64} & 2.7e-17 & 3.6e-12\\\hline % p=52
\textbf{80} & 6.7e-21 & 8.9e-16\\\hline % p=64
\textbf{128} & 3.2e-30 & 2.4e-35\\\hline % p=112
\end{tabular}
\caption{$ulp(c_i)$}
\end{table}

We can thus make the approximation $sin(6\times ulp(c_i))\approx 6\times ulp(c_i)$. The total error of $cos(b_i+c_i\times t)$ is thus $\approx 6\times ulp(c_i)$.

Now, we can say $ulp(a_i) \ll ulp(c_i)$ and neglect it, and the total error of a basic operation is thus $a_i\times 6\times ulp(c_i)$. To get the biggest bound, we can use $a_i<0.1$ and thus $e_a<-2$, so expanding everything we have 
$$E_{basicop} < 2^{-2}\times 6 \times 2^{e_c-p+1} < 2^{e_c-p+2}$$

Now, having $a_i<\frac{a_1}{100}$ for $i>1$, we can say that if the sum is made with the Kahan formula, the total error of the sum can be approximated by the error of the first basic operation.

There is the same kind of reduction applying for the polynomial. If we note it in the form $s_0+s_1\times t+s_2\times t^2+s_3\times s_3+s_4\times t^4+s_5\times t^5$, where $s_0, s_1$, etc. are the result of the summations of the basic operations. We shall also remember that $2<t<9$. 

As an approximation, we can say that $s_i$ is approximately the first term of the sum, and that the first term of the sum is bound by the corresponding $a_1$ (as $cos(x)\leq 1$). 

Looking at the numbers of the vsop87 files, we can see that $a_1 of s_1 \approx \frac{a_1 of s_0}{10}$, $a_1 of s_2\approx \frac{a_1 of s_1}{10}$, etc. so we can approximate $s_1 \approx \frac{s_0}{10}$, $s_2\approx \frac{s_1}{10}$, etc. So if we want to bind the error, we can take $t=10$, and say that all terms of the polynomial are about the same size\footnote{note that this is not true for calendrical calculations around year 2000, which will thus hold more precise results.}. So all the errors of the different terms will be summed.

Now computing the error on the term $s_i\times t^i$. We have to come back to the error of the first basic operation of $s_i$, which is $< a_i\times 6\times ulp(c_i)$. As $a_1 of s_1\approx \frac{a_1 of s_0}{10}$, etc. we can bind $a_i$ by $10^{-i-1}$, but if we multiply it by $t^i$ with $t\approx 10$, the bound will be $10^{-1}$ and thus be the same as the first sum, for which we have binded the error by $E_{basicop} < 2^{-2}\times 6 \times 2^{e_c-p+1}$. To get the worse situation, we can say that $e_c$ is its maximum: $e_c=13$.So the total error of the polynomial evaluation will be bounded by 5\times the error on one term, which means a total error bound for a vsop87 coordinate of:

$$E_{vsop87} < 5 \times 2^{-2}\times 6 \times 2^{e_c-p+1} < 2^{e_c-p+4} < 2^{17-p}$$

Converting this error bound in seconds, we can compute the following table:

\begin{table}[h]
\centering
\sisetup{table-format=1.1, table-figures-exponent = 2}
\begin{tabular}{|c|S|}
\hline
\textbf{Nb. of bits} & \multicolumn{1}{c|}{\textbf{Error bound (in s)}} \\\hline
\textbf{32} & 3.2e3 \\\hline % p=23
\textbf{64} & 6e-6 \\\hline % p=52
\textbf{80} & 1.4e-10 \\\hline % p=64
\textbf{128} & 5.2e-24 \\\hline % p=112
\end{tabular}
\caption{Maximum error of vsop87 coordinate due to floating point operations}
\end{table}

So, looking at table \ref{vsopprecision}, we can conclude that 64-bit internal operations is enough for vsop87 calculations. From the previous calculations, we can also say that computing the first basic operation of each group with more precision will drastically improve the precision of the whole calculation (almost giving it the improved precision).

\mysubsection{Measuring errors}

\mysubsubsection{Simple experiments}

\mysubsubsection{Verfication tools}

Some tools are being created to measure the accuracy of floating point programs, by directly looking into the assembler code. The Frama-C\footnote{\url{http://frama-c.com}} framework (with the WP Plugin\footnote{\url{http://frama-c.com/wp.html}}) and the Why3\footnote{http://why3.lri.fr/} framework are good examples, though not easy to use. These require the use of ACSL annotations \cite{ACSL}\footnote{For floating point numbers, see section 2.2.5.}.
% TODO: ACSL : http://frama-c.com/download/acsl.pdf


